plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
getwd()
knitr::opts_chunk$set(echo = TRUE, fig.path='/Users/emilyjin/Desktop/shifanjin-repo/hw04/images')
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
source(file = '../code/archive-functions.R')
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
source(file = '../code/archive-functions.R')
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
source(file = '../code/archive-functions.R')
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
raw_data_dplyr <- read_archive("dplyr")
clean_data_dplyr <- clean_archive(raw_data_dplyr)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data_dplyr, file = '~/Desktop/shifanjin-repo/hw04/data/dplyr-archive.csv')
raw_data_ggplot2 <- read_archive("ggplot2")
clean_data_ggplot2 <- clean_archive(raw_data_ggplot2)
write.csv(clean_data_ggplot2, file = '~/Desktop/shifanjin-repo/hw04/data/ggplot2-archive.csv')
raw_data_xml <- read_archive("XML")
clean_data_xml <- clean_archive(raw_data_xml)
write.csv(clean_data_xml, file = '~/Desktop/shifanjin-repo/hw04/data/xml-archive.csv')
raw_data_knitr <- read_archive("knitr")
clean_data_knitr <- clean_archive(raw_data_knitr)
write.csv(clean_data_knitr, file = '~/Desktop/shifanjin-repo/hw04/data/knitr-archive.csv')
all_tbl <- rbind(clean_data_dplyr, clean_data_ggplot2, clean_data_knitr, clean_data_xml)
ggplot(data = all_tbl) +
geom_step(aes(x = date, y = size, color = name), size= 1) +
ylab('size(Kilobytes)') + xlab('date')
ggplot(data = all_tbl) +
geom_step(aes(x = date, y = size, color = name), size= 1) +
facet_wrap(~ name,scales='free') +
ylab('size(Kilobytes)') + xlab('date')
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
version_sizes <- function(org_tbl) {
size = str_replace(org_tbl$Size, pattern = 'K|M|G', replacement = '')
size = as.numeric(size)
#Caution: some packages have size
#values expressed in MB, these must be converted to KB
for(i in 1 : length(org_tbl$Size)) {
if (str_sub(org_tbl$Size, start = -1)[i] == 'M') {
size[i] = 1000 * size[i]
}
}
return(size)
}
source(file = '../code/archive-functions.R')
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
clean_data
version_sizes(raw_data)
#extracts the size of the version
version_sizes <- function(org_tbl) {
size = str_replace(org_tbl$Size, pattern = 'K|M|G', replacement = '')
size = as.numeric(size)
#Caution: some packages have size
#values expressed in MB, these must be converted to KB
for(i in 1 : length(org_tbl$Size)) {
if (str_sub(org_tbl$Size, start = -1)[i] == 'M') {
size[i] = 1000 * size[i]
}
}
return(size)
}
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_df = as.data.frame(content)
View(content_df)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
str_detect(content, pattern = "@")
View(content_df)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_detect(content, pattern = "@")
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content, pattern = "@")
content_at
str(conte)
str(content_at)
content_at = str_extract(content_at, pattern = "@")
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
source(file = '../code/archive-functions.R')
source(file = "../code/regex_functions.R")
source(file = "../code/regex-functions.R")
split_chars('Go Bears!')
split_chars('Expecto Patronum')
vec = split_chars('Go Bears!')
num_vowels(vec)
count_vowels("The quick brown fox jumps over the lazy dog")
count_vowels("THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG")
count_vowels("The quick brown fox jumps over the lazy dog")
count_vowels("THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG")
count_vowels("THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG")
reverse_chars("gattaca")
reverse_chars("Lumox Maxima")
reverse_words("sentence! this reverse")
reverse_words("string")
vec = split_chars('Go Bears!')
split_chars('Expecto Patronum')
num_vowels(vec)
count_vowels("The quick brown fox jumps over the lazy dog")
count_vowels("THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG")
reverse_chars("gattaca")
reverse_chars("Lumox Maxima")
reverse_words("sentence! this reverse")
reverse_words("string")
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content_df, pattern = "@")
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
View(content_at)
content_word_vec = str_split(content_at, pattern = " ")
View(content_word_vec)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content, pattern = "@")
#content_at = as.data.frame(content_at)
content_word_vec = str_split(content_at, pattern = " ")
View(content_word_vec)
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
content_word_vec = str_split(content_at, pattern = " ")
content_word_vec = str_split(content_at, pattern = " ")
View(content_word_vec)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
View(content_at)
content_word_vec = str_split(content_at, pattern = " ")
View(content_word_vec)
content_word_vec
content_word_vec = c()
for (i in 1:length(content_at)) {
content_word_vec[i] = str_split(content_at[i], pattern = " ")
}
View(content_word_vec)
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
content_word_vec = c()
for (i in 1:length(content_at)) {
content_word_vec[i] = str_split(content_at[i], pattern = " ")
}
View(content_word_vec)
content_word_vec = data.frame()
for (i in 1:length(content_at)) {
content_word_vec = str_split(content_at[i], pattern = " ")
}
#content
View(content_word_vec)
#content_word_vec = str_split(content_at, pattern = " ")
str_split(content_at[1], pattern = " ")
content_at = as.data.frame(content_at)
View(content_at)
content_at[1]
View(content_at)
#content_word_vec = str_split(content_at, pattern = " ")
str_split(content_at$content_at[1], pattern = " ")
for (i in 1:length(content_at)) {
content_word_vec = str_split(content_at$content_at[i], pattern = " ")
}
View(content_word_vec)
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
content_word_vec = data.frame()
for (i in 1:length(content_at)) {
content_word_vec = str_split(content_at$content_at[i], pattern = " ")
}
View(content_word_vec)
length(content_at)
for (i in 1:length(content_at$content_at)) {
content_word_vec = str_split(content_at$content_at[i], pattern = " ")
}
View(content_word_vec)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
content_at = str_subset(content, pattern = "@")
content_at = as.data.frame(content_at)
content_word_vec = c()
for (i in 1:length(content_at$content_at)) {
content_word_vec[i] = str_split(content_at$content_at[i], pattern = " ")
}
#content_word_vec = str_
View(content_word_vec)
table = as.data.frame(table(str_count(content, pattern = '@\\w+')))
View(table)
str_count(content, pattern = '@\\w+')
table = as.data.frame(table(str_count(content, pattern = '@\\w+')))
table = as.data.frame(table(str_count(content, pattern = '@\\w+')))
str_count(content, pattern = '@\\w+')
table(str_count(content, pattern = '@\\w+'))
count_at = str_count(content, pattern = '@\\w+')
table = as.data.frame(table(count_at))
table
barplot(table$Freq)
#Also write code to display the content of the tweet with 10 mentions.
max(count_at)
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
index
content[index]
# ===================================================================
# Title: regex-functions.R
# Description:
#   This script is basically writing functions to deal with string
#   and characters
# Author: Shi Fan Jin
# Date: 4-11-2018
# ===================================================================
library(stringr)
library(dplyr)
#' @title single character elements
#' @description takes a character string, and splits the content into one single character elements.
#' @param str the string to be split
#' @return the single charater elements
split_chars <- function(str) {
ele = str_split(str, "")
ele = unlist(ele)
return(ele)
}
#' @title number of vowels
#' @description find the number of vowels in a character vector
#' @param ch_vec a vector in which each element is a single character
#' @return the number of vowels
num_vowels <- function(ch_vec) {
count_a <- sum(grepl('a',ch_vec,ignore.case = TRUE))
count_e <- sum(grepl('e',ch_vec,ignore.case = TRUE))
count_i <- sum(grepl('i',ch_vec,ignore.case = TRUE))
count_o <- sum(grepl('o',ch_vec,ignore.case = TRUE))
count_u <- sum(grepl('u',ch_vec,ignore.case = TRUE))
table <- c(count_a,count_e,count_i,count_o,count_u)
names(table) <- c("a", "e", "i", "o", "u")
return(table)
}
#' @title count vowels
#' @description computes the number of vowels of a character string
#' @param ch_str a character string
#' @return the number of vowels
count_vowels <- function(ch_str) {
ch_vec = split_chars(ch_str)
tbl = num_vowels(ch_vec)
return(tbl)
}
count_vowels('Hello Emily!')
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
table_hash <- as.data.frame(table(str_count((data$content), pattern = '#[[:alpha:]][[:alnum:]]*')))
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
library(stringr)
library(dplyr)
library(ggplot2)
source(file = '../code/archive-functions.R')
table_hash <- as.data.frame(table(str_count((data$content), pattern = '#[[:alpha:]][[:alnum:]]*')))
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
table_hash <- as.data.frame(table(str_count((content), pattern = '#[[:alpha:]][[:alnum:]]*')))
str_count(content, pattern = '#[[:alpha:]][[:alnum:]]*')
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
count_at = str_count(content, pattern = '@\\w+')
# Display such frequencies, and make a barplot of these counts (i.e. number of tweets
# with 0 mentions, with 1 mention, with 2 mentions, etc).
table_at = as.data.frame(table(count_at))
table_at
barplot(table_at$Freq)
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
content[index]
View(table_at)
table_hash = as.data.frame(table(count_hash))
count_hash = str_count(content, pattern = '#[[:alpha:]][[:alnum:]]*')
table_hash = as.data.frame(table(count_hash))
table_hash
barplot(table_hash$Freq)
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
count_at = str_count(content, pattern = '@\\w+')
# Display such frequencies, and make a barplot of these counts (i.e. number of tweets
# with 0 mentions, with 1 mention, with 2 mentions, etc).
table_at = as.data.frame(table(count_at))
table_at
barplot(table_at$Freq)
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
content[index]
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
char_num = unlist(char_num)
char_num = unlist(char_num)
char_num = lapply(content, nchar)
View(char_num)
char_num = unlist(char_num)
char_num
content
char_num = lapply(content, nchar)
char_num = lapply(content, nchar)
summary(char_num)
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
count_at = str_count(content, pattern = '@\\w+')
library(stringr)
library(dplyr)
library(ggplot2)
source(file = '../code/archive-functions.R')
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
count_at = str_count(content, pattern = '@\\w+')
# Display such frequencies, and make a barplot of these counts (i.e. number of tweets
# with 0 mentions, with 1 mention, with 2 mentions, etc).
table_at = as.data.frame(table(count_at))
table_at
barplot(table_at$Freq)
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
content[index]
count_at
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
content[index]
index
content[index]
str_extract(content, patter = "#")
str_extract(content, patter = "#[[:alpha:]][[:alnum:]]*")
str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*")
unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(all_hash) - 1
all_hash
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
mode_hash = mode(nchar(all_hash)) - 1
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
mode_hash = mode(nchar(all_hash)) - 1
mode_hash = mode(nchar(all_hash)) - 1
nchar(all_hash)
hash_length = nchar(all_hash) - 1
table(hash_length)
max(table(hash_length))
max(table(hash_length))
max(table(hash_length))
table(hash_length)
max(table(hash_length))
max(table(hash_length))
table(hash_length)
hash_freq = table(hash_length)
hash_freq
hash_freq = as.data.frame(hash_freq)
hash_freq
max(hash_freq)
max(hash_freq$Freq)
which.max(hash_freq$Freq)
hash_freq
max(hash_freq$Freq)
hash_length = nchar(all_hash) - 1
hash_freq = table(hash_length)
hash_freq = as.data.frame(hash_freq)
max_hash = max(hash_freq$Freq)
hash_freq[, which(hash_freq$Freq == max_hash)]
max_hash = max(hash_freq$Freq)
max_hash
max_hash
print("The mode of the hashtags length is 4 and 9, where there are", max_hash, "for each 4 and 9")
print("The mode of the hashtags length is 4 and 9, where there are 97 for each 4 and 9")
hash_length = nchar(all_hash) - 1
hash_freq = table(hash_length)
hash_freq = as.data.frame(hash_freq)
max_hash = max(hash_freq$Freq)
max_hash
print("The mode of the length of the hashtags are 4 and 9, where there are 97 for each 4 and 9")
max_hash = max(hash_freq)
hash_length = nchar(all_hash) - 1
hash_freq = table(hash_length)
max_hash = max(hash_freq)
max_hash
print("The mode of the length of the hashtags are 4 and 9, where there are 97 for each 4 and 9")
hash_freq
hash_length = nchar(all_hash) - 1
hash_freq = table(hash_length)
hash_freq
max_hash = max(hash_freq)
max_hash
print("The mode of the length of the hashtags are 4 and 9, where there are 97 for each 4 and 9")
source(file = "../code/regex-functions.R")
source(file = "../code/regex-functions.R")
knitr::opts_chunk$set(echo = TRUE, fig.path='../images/')
library(stringr)
library(dplyr)
library(ggplot2)
source(file = '../code/archive-functions.R')
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data, file = '~/Desktop/shifanjin-repo/hw04/data/stringr-archive.csv')
raw_data_dplyr <- read_archive("dplyr")
clean_data_dplyr <- clean_archive(raw_data_dplyr)
#Export the table as a CSV file "stringr-archive.csv" to the data/ folder.
write.csv(clean_data_dplyr, file = '~/Desktop/shifanjin-repo/hw04/data/dplyr-archive.csv')
raw_data_ggplot2 <- read_archive("ggplot2")
clean_data_ggplot2 <- clean_archive(raw_data_ggplot2)
write.csv(clean_data_ggplot2, file = '~/Desktop/shifanjin-repo/hw04/data/ggplot2-archive.csv')
raw_data_xml <- read_archive("XML")
clean_data_xml <- clean_archive(raw_data_xml)
write.csv(clean_data_xml, file = '~/Desktop/shifanjin-repo/hw04/data/xml-archive.csv')
raw_data_knitr <- read_archive("knitr")
clean_data_knitr <- clean_archive(raw_data_knitr)
write.csv(clean_data_knitr, file = '~/Desktop/shifanjin-repo/hw04/data/knitr-archive.csv')
all_tbl <- rbind(clean_data_dplyr, clean_data_ggplot2, clean_data_knitr, clean_data_xml)
ggplot(data = all_tbl) +
geom_step(aes(x = date, y = size, color = name), size= 1) +
ylab('size(Kilobytes)') + xlab('date')
ggplot(data = all_tbl) +
geom_step(aes(x = date, y = size, color = name), size= 1) +
facet_wrap(~ name,scales='free') +
ylab('size(Kilobytes)') + xlab('date')
source(file = "../code/regex-functions.R")
vec = split_chars('Go Bears!')
split_chars('Expecto Patronum')
num_vowels(vec)
count_vowels("The quick brown fox jumps over the lazy dog")
count_vowels("THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG")
reverse_chars("gattaca")
reverse_chars("Lumox Maxima")
reverse_words("sentence! this reverse")
reverse_words("string")
emotion = read.csv(file = "../data/text-emotion.csv", stringsAsFactors = FALSE)
content = emotion$content
str(content)
char_num = lapply(content, nchar)
char_num = unlist(char_num)
summary(char_num)
hist(char_num, main = "Histogram of the characters in each content", xlab = "number of characters", breaks = seq(from = 1, to = 175, by = 5))
# Count the number of @ mentions (i.e. mention to another user) in the tweet contents.
count_at = str_count(content, pattern = '@\\w+')
# Display such frequencies, and make a barplot of these counts (i.e. number of tweets
# with 0 mentions, with 1 mention, with 2 mentions, etc).
table_at = as.data.frame(table(count_at))
table_at
barplot(table_at$Freq)
#Also write code to display the content of the tweet with 10 mentions.
index = which.max(count_at)
content[index]
count_hash = str_count(content, pattern = '#[[:alpha:]][[:alnum:]]*')
table_hash = as.data.frame(table(count_hash))
barplot(table_hash$Freq)
all_hash<- unlist(str_extract_all(content, patter = "#[[:alpha:]][[:alnum:]]*"))
ave_hash = mean(nchar(all_hash)) - 1
ave_hash
hash_length = nchar(all_hash) - 1
hash_freq = table(hash_length)
hash_freq
ssmax_hash = max(hash_freq)
max_hash
print("The mode of the length of the hashtags are 4 and 9, where there are 97 for each 4 and 9")
vec
